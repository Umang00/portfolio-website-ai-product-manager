{"filename":"journey_fy-2025-2026.pdf","documentType":"journey","stats":{"totalChunks":34,"avgChunkSize":422,"minChunkSize":247,"maxChunkSize":535,"avgOverlap":13,"totalTokens":14333},"quality":{"filename":"journey_fy-2025-2026.pdf","issues":["Chunk 0 may have mid-sentence cut","Chunk 1 may have mid-sentence cut","Chunk 5 may have mid-sentence cut","Chunk 7 may have mid-sentence cut","Chunk 8 may have mid-sentence cut","Chunk 9 may have mid-sentence cut","Chunk 10 may have mid-sentence cut","Chunk 13 may have mid-sentence cut","Chunk 15 may have mid-sentence cut","Chunk 16 may have mid-sentence cut","Chunk 18 may have mid-sentence cut","Chunk 19 may have mid-sentence cut","Chunk 20 may have mid-sentence cut","Chunk 22 may have mid-sentence cut","Chunk 23 may have mid-sentence cut","Chunk 24 may have mid-sentence cut","Chunk 27 may have mid-sentence cut","Chunk 29 may have mid-sentence cut","Chunk 31 may have mid-sentence cut","Chunk 32 may have mid-sentence cut"],"warnings":[],"passed":false},"sampleChunks":[{"text":"FY 25-26\nMy brief work story of FY 25-26\nAt the start of the new financial year I reached a turning\npoint. After months of spending my weekends experimenting with\nAI‚Äîbuilding prototypes, stress-testing new tools, and becoming\nthe de facto ‚ÄúAI guy‚Äù at the office‚ÄîI realised it was time to go\nall-in. I...","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"1-1","partInfo":"Part 1 of 34"},"chunkIndex":0,"boundaries":{"startChar":2,"endChar":1852,"overlapWords":0,"splitAt":"fixed_size"},"tokenCount":398},{"text":"With Redshift SQL, I could dive into\nraw data and stitch together exactly the queries I needed For\n\nexample, identifying a thousand new users and then isolating the\nhundred who were most engaged, tracing back how they were\nacquired, and what specific early experiences drove that\nengagement. Was it s...","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"2-2","partInfo":"Part 2 of 34"},"chunkIndex":1,"boundaries":{"startChar":0,"endChar":2260,"overlapWords":1,"splitAt":"paragraph_break"},"tokenCount":472},{"text":"me the best of both worlds: easy visual dashboards for broad patterns, and deep raw-data analysis for the tougher questions that shaped product direction. Realistic Image Generation with Character Consistency\n\nIn the dating space, one unavoidable reality is that most\ncompanies deploy bots‚Äîespecially...","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"3-3","partInfo":"Part 3 of 34"},"chunkIndex":2,"boundaries":{"startChar":0,"endChar":2477,"overlapWords":30,"splitAt":"paragraph_break"},"tokenCount":492},{"text":"There were technical\nhiccups (one recording had noise, another lost the screen), but\n\nI even went the extra mile to sync audio and video later, so at\nleast one complete reference would exist.\nBy the time I was done, I felt confident I had left no gaps.\nBetween the Retool documentation, the content h...","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"34-34","partInfo":"Part 34 of 34"},"chunkIndex":33,"boundaries":{"startChar":62992,"endChar":64484,"overlapWords":13,"splitAt":"paragraph_break"},"tokenCount":332}],"fullChunks":[{"text":"FY 25-26\nMy brief work story of FY 25-26\nAt the start of the new financial year I reached a turning\npoint. After months of spending my weekends experimenting with\nAI‚Äîbuilding prototypes, stress-testing new tools, and becoming\nthe de facto ‚ÄúAI guy‚Äù at the office‚ÄîI realised it was time to go\nall-in. I resigned so I could focus full-time on advanced AI\nwork and get back to hands-on coding, determined to understand\nhow these systems run at a fundamental level and refine my vibe\ncoding skills.\nThe quarter wasn‚Äôt easy. My health took a significant hit, yet I\nstill led several critical projects and pushed to ship them\nbefore my notice period ended. Juggling recovery,\nknowledge-transfer sessions, and final feature launches was\nexhausting, but it validated my ability to deliver under\npressure.\nLooking back, those three months were intense but deeply\nrewarding: I closed out key initiatives, ensured a clean\nhand-off for the team, and laid the groundwork for my next\nchapter in AI.\nI‚Äôll start by detailing the smaller analytics tasks I wrapped up\nduring this period, then move on to the larger, high-impact\nprojects that truly defined this phase of my career.\nData Analysis via Redshift SQL & Mixpanel\nFor several quarters, I had been actively learning and applying\nSQL in my work‚Äîmainly through the help of GPT. This gave me the\nability to run analyses independently, without relying on the\ndata team. It sped up decision-making and gave me more\nownership: instead of waiting for someone else to pull numbers,\nI could directly test hypotheses, validate assumptions, and back\nmy calls with data.\nPreviously, our team mostly worked with Mixpanel dashboards.\nThey were useful but had limits‚Äîespecially when it came to more\ncomplex, nuanced questions. With Redshift SQL, I could dive into\nraw data and stitch together exactly the queries I needed. For","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"1-1","partInfo":"Part 1 of 34"},"chunkIndex":0,"boundaries":{"startChar":2,"endChar":1852,"overlapWords":0,"splitAt":"fixed_size"},"tokenCount":398},{"text":"With Redshift SQL, I could dive into\nraw data and stitch together exactly the queries I needed For\n\nexample, identifying a thousand new users and then isolating the\nhundred who were most engaged, tracing back how they were\nacquired, and what specific early experiences drove that\nengagement. Was it strong profile views in the first few days?\nOr something else entirely? SQL allowed me to find these answers\nin a structured way.\nThe benefit was clear when looking at match rates. With SQL, I\ncould ask: What‚Äôs the match rate for new users from a specific\nregion, acquired through a specific channel, within their first\ntwo or seven days? How does that compare over time? We could\nthen see whether more time on the app improved matches or\nworsened them‚Äîespecially in cases where gender imbalances might\naffect outcomes.\nAnother area was analyzing user conversations. While Mixpanel\nshowed conversion events, it didn‚Äôt capture the sentiment or\nsequence of exchanges. Sendbird captured the sequence of\nexchanges but didn‚Äôt capture the sentiment and it could be hard\nto visualize and draw down insights from it. So the best option\nhere is to write queries that mapped back-and-forth\nchats‚Äîensuring ‚ÄúA to B‚Äù and ‚ÄúB to A‚Äù counted as the same\nconversation‚ÄîI could build clean datasets for the last 7 or 30\ndays. These datasets could then be run through LLMs to extract\ninsights on conversation quality, tone, and opportunities to\nimprove user experience.\nWorking this way built my muscle for handling complex queries:\ndefining logic carefully, cross-checking outputs, and validating\ndata against actual user behavior. I learned quickly that a SQL\nquery can still run even if the logic is wrong‚Äîso I made it a\nhabit to validate results by tracking specific users‚Äô activities\nto ensure the data matched reality.\nThis hands-on work became central to my role in the strategy\nteam. Our day-to-day revolved around building strategies,\nsuggesting new features, and improving existing ones. For that,\ndata was the cornerstone. Redshift SQL combined with Mixpanel\ngave me the best of both worlds: easy visual dashboards for\nbroad patterns, and deep raw-data analysis for the tougher\nquestions that shaped product direction.\nRealistic Image Generation with Character Consistency","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"2-2","partInfo":"Part 2 of 34"},"chunkIndex":1,"boundaries":{"startChar":0,"endChar":2260,"overlapWords":1,"splitAt":"paragraph_break"},"tokenCount":472},{"text":"me the best of both worlds: easy visual dashboards for broad patterns, and deep raw-data analysis for the tougher questions that shaped product direction. Realistic Image Generation with Character Consistency\n\nIn the dating space, one unavoidable reality is that most\ncompanies deploy bots‚Äîespecially in the early stages when demand\nand supply don‚Äôt balance. For example, male users often\noutnumber female users, and acquiring genuine female users can\nbe both costly and limited. If the app can‚Äôt provide enough\nengagement early on, users leave. To maintain balance, we\nexplored creating bots‚Äîparticularly female bots‚Äîthat could\nsustain initial interest until real users filled the gap.\nTo make these bots convincing, we needed realistic images. One\nimage wasn‚Äôt enough because our platform required every user to\nupload at least three. If a bot only had one image, it would\nimmediately look suspicious. So I started experimenting with\nmultiple approaches: GPT-4o‚Äôs image capabilities, FAL,\nReplicate, and even Kling (which our marketing team was already\ntesting for video generation). While these tools produced good\nsingle images, they didn‚Äôt solve the multi-image consistency\nproblem.\nThat‚Äôs when I dove deeper into open-source models like FluxDev\nand FluxPro, which excelled at realism. Flux‚Äôs results were\nimpressive, but again, generating consistent sets of images for\nthe same persona was difficult. The breakthrough came with seed\nlocking. I discovered that by fixing the seed in Google‚Äôs\nImageGen (AI Studio), I could regenerate variations of the same\ncharacter with much better consistency. This wasn‚Äôt available in\nIndia at the time, so I used a VPN to access it via the US,\ntested different prompts, and finally achieved more controlled\noutputs.\nThis approach worked reasonably well for female personas, though\nmale image consistency remained more challenging. Since I had\nmultiple priorities, I trained an intern to handle much of the\nexecution. I prepared detailed prompts‚Äîwhere she only needed to\nswap locations like ‚Äúmountains‚Äù, ‚Äúbeach‚Äù, ‚Äúcafe‚Äù, etc ‚Äîto\ngenerate believable variations of the same character.\nI documented the entire method: the seed-locking trick, prompt\nstructures, and a library of generated images. I also shared my\nlearnings with the founders and other teams. Interestingly, they\nhadn‚Äôt been aware of the seed technique and were surprised by\nhow effective it was, though the manual nature of the process\n(and lack of APIs) limited automation.","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"3-3","partInfo":"Part 3 of 34"},"chunkIndex":2,"boundaries":{"startChar":0,"endChar":2477,"overlapWords":30,"splitAt":"paragraph_break"},"tokenCount":492},{"text":"Interestingly, they\nhadn‚Äôt been aware of the seed technique and were surprised by\nhow effective it was, though the manual nature of the process\n(and lack of APIs) limited automation.\n\nWe also explored other approaches like face-swapping to expand\nimage sets, but those attempts often introduced artifacts and\nlooked visibly AI-generated. Ultimately, I recognized that\nsolving this problem fully would require more dedicated time\nthan I could give. Instead, I handed off my research,\nexperiments, and resources to the team members focusing on it\nfull-time.\nThis project was a fascinating look at the cutting edge of\nrealistic, consistent image generation for AI personas‚Äîbalancing\ncreativity, technical experimentation, and practical\nlimitations.\nFYI: This was done way before models like Flux Krea Dev, Nano\nBanana, Seedream, etc were released.\nPush Notifications for Male Users When a New Female Joins\nNearby\nOne of the challenges in the dating space is maintaining male\nengagement, especially since male users typically outnumber\nfemales. To address this, I proposed an experiment: whenever a\nnew female user joined within a certain radius‚Äîsay 200 miles of\na male‚Äôs signup location‚Äîwe would send a push notification to\nrelevant male users.\nThe logic was straightforward: if a male in California had\nsigned up, and a new female joined within 200 miles, he‚Äôd get a\nnotification. But we added conditions to keep the experience\nrelevant: both users‚Äô age preferences had to align, the male\naccount had to be active (not deleted), and other dating\npreference filters needed to match. The idea was to give males\nan incentive to check the app more often, while also increasing\nvisibility for new female users, ensuring they received more\nrequests and attention early on.\nI drafted a full proposal outlining the conditions, potential\nbenefits, and experiment setup. My manager liked the idea, and\nwe aligned with the marketing team, who handled PN campaigns.\nThe tech and product teams also agreed in principle but asked\nfor supporting data before implementation. Specifically, they\nwanted to know: if we ran this experiment, how many\nnotifications would actually be triggered daily? The concern was\navoiding notification fatigue‚Äîsending too many PNs to the same\nuser could backfire.","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"4-4","partInfo":"Part 4 of 34"},"chunkIndex":3,"boundaries":{"startChar":6103,"endChar":8379,"overlapWords":30,"splitAt":"paragraph_break"},"tokenCount":462},{"text":"The concern was\navoiding notification fatigue‚Äîsending too many PNs to the same\nuser could backfire.\n\nI designed SQL queries to model this. Each condition (location\nradius, age preferences, activity status, etc.) required mapping\nspecific data points from different tables. This meant writing\ncomplex joins and carefully sequencing filters to avoid logic\nerrors. I learned firsthand that the order of filtering matters:\napplying a condition globally versus at the bottom of a query\ncan completely change outputs. To validate results, I\ncross-checked with sample users manually‚Äîfor instance,\nconfirming that if the system said ‚ÄúRhea‚Äù should trigger a\nnotification for ‚ÄúAman,‚Äù the distance, preferences, and activity\nstatus all matched correctly.\nOnce the logic was sound, I presented the projected numbers: how\nmany notifications would be sent daily, how caps could be\napplied (we suggested a ceiling of 5‚Äì10 PNs per user across all\njourneys), and how this would balance engagement without\nspamming. From there, the tech and product teams implemented the\nlogic into code, building it directly into the PN framework.\nThe results were encouraging. Users who previously weren‚Äôt\nactive began opening the app again, at least to ‚Äúcheck casually‚Äù\nwhen notified about a new female nearby. Some engaged further by\nsending requests or matches, closing the loop we had originally\nhypothesized.\nThis experiment showed how data-backed logic, combined with user\nbehavior insights, could directly improve engagement and create\na healthier balance on the app.\nPersonality Trait Model\nAs FY 24‚Äì25 began, some of the foundational research I had done\nearlier started turning into live features on the app. One of\nthe most visible examples was the integration of MBTI-based\npersonality matching.\nWe introduced a system where every user received their MBTI type\nand, based on it, the app highlighted strong matches, neutral\nmatches, and weaker matches. This logic was directly inspired by\nmy earlier research into the Personality Trait Models like 16\nPersonality Test, Marriage Pact, MBTI, Boo‚Äôs Personality Test\nAlgorithm, etc where I explored how personality traits could be\nmapped into compatibility patterns. That framework gave us the\nstructure to translate MBTI into meaningful relationship\npairings.","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"5-5","partInfo":"Part 5 of 34"},"chunkIndex":4,"boundaries":{"startChar":8280,"endChar":10563,"overlapWords":15,"splitAt":"paragraph_break"},"tokenCount":450},{"text":"That framework gave us the\nstructure to translate MBTI into meaningful relationship\npairings.\n\nAlthough I wasn‚Äôt hands-on coding this feature, it was\ngratifying to see my research guiding product direction. The\npositioning of our app shifted: instead of being ‚Äújust another\ndating app,‚Äù we could now present ourselves as a platform that\nleveraged personality science for deeper, more thoughtful\nmatching.\nThe implementation required blending MBTI pair logic into the\nbroader matchmaking score formula. While distance and\npreferences still mattered, the data team expanded the backend\nto incorporate behavioral patterns in how users answered MBTI\nquestions. My earlier experiments‚Äîwhere I tested different\nformulas, hypotheses, and components for scoring‚Äîfed directly\ninto this work.\nBy the time I resigned, MBTI-based polls were live. Users had\nMBTI tags on their profiles, and matches reflected compatibility\nbeyond surface-level filters. This improved both transparency\nand trust: users could see there was a structured,\npersonality-driven logic behind why they were matched, not just\nsimple filters like distance or age.\nFor me, this was a milestone‚Äîwatching long-term research evolve\ninto a tangible feature that strengthened our product\npositioning and enhanced the user experience.\nRed Flags You Both Are Okay With: Shortvibes for MBTI Polls\nOnce we decided to roll out MBTI-based polls and assign\npersonality tags to users, the next challenge was how to\ngenerate short vibes (the one-liner connections shown between\nusers) for these polls. Using our usual formats‚Äîfirst date\nideas, media recommendations, or debate topics‚Äîposed risks.\nSince every new user would answer the same fixed set of MBTI\nquestions, most people would converge on the top two options in\neach poll. We had already seen this pattern in earlier fixed and\npromoted poll campaigns: typically, one or two options received\n80‚Äì90% of the votes, while the remaining ones barely reached\n5‚Äì10%.\nIf we continued generating short vibes solely from those common\noptions, the experience would quickly become repetitive and\nstale. To avoid that, I explored alternatives. Options included","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"6-6","partInfo":"Part 6 of 34"},"chunkIndex":5,"boundaries":{"startChar":10470,"endChar":12621,"overlapWords":13,"splitAt":"paragraph_break"},"tokenCount":419},{"text":"To avoid that, I explored alternatives Options included\n\nshowcasing green flags, red flags, etc ‚Äîbut the one that struck\nthe best balance was ‚ÄúRed flags you both are okay with.‚Äù\nThis worked for two reasons. First, it gave users fresh,\nengaging one-liners tied to their MBTI answers without\noverwhelming them with an entirely new experience. Second, it\npreserved continuity for older users who were already familiar\nwith the short vibe format. By prioritizing red flags while\nstill keeping a mix of other formats, we could reduce repetition\nwhile maintaining variety.\nTo test this, I set up an automation system to generate red-flag\nshort vibes. Instead of waiting for the tech team (who had\nlimited resources at the time), I replicated the formula we had\nused earlier for date ideas and debate topics. I wrote an App\nScript that pulled MBTI poll questions and options from Google\nSheets, generated the one-liners row by row, and wrote them back\ninto the sheet. Since App Script wasn‚Äôt easy for everyone to\nhandle, I also built a version on Make.com, which was more\naccessible for the team.\nThe process worked like this: each question and its four options\nwere entered into Google Sheets, with each option listed as a\nseparate row. The system prompt then generated the red-flag\nshort vibe for that row. The outputs were stored directly in the\nsheet, creating a lightweight but reliable workflow. This\nmirrored the automation we had already used for earlier short\nvibe categories, making adoption smooth.\nAfter validating the outputs and getting team buy-in, we handed\nthe system to the tech team for final implementation. They\ndidn‚Äôt need to reinvent the generation logic‚Äîonly to handle\ndetails like randomizing which of the four options appeared and\nprioritizing red flags within the MBTI polls.\nThe experiment was a success. By introducing ‚ÄúRed flags you both\nare okay with,‚Äù we reduced the repetition issue, kept short\nvibes interesting, and gave users something more relatable and\npersonal. Click-through rates improved, and users engaged more\nwith their matches compared to previous versions of short vibes.\nThis feature started as a stopgap solution to prevent repetition\nbut ended up becoming a strong addition to the app‚Äôs MBTI\nexperience.","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"7-7","partInfo":"Part 7 of 34"},"chunkIndex":6,"boundaries":{"startChar":0,"endChar":2245,"overlapWords":1,"splitAt":"paragraph_break"},"tokenCount":475},{"text":"This feature started as a stopgap solution to prevent repetition\nbut ended up becoming a strong addition to the app‚Äôs MBTI\nexperience.\n\nShortVibes based on First Date Ideas, Debate Topics & Media\nRecommendations\nIn FY 25‚Äì26, I followed through on an initiative to replace our\nastrology-based shortvibes with something more personal and less\nrepetitive: first date ideas, debate topics, and media\nrecommendations (YouTube, Netflix, Prime, Spotify).\nThe astrology system relied on sun signs and a small repository\nof prewritten one-liners (e.g., ‚ÄúTaurus √ó Taurus‚Äù), which led to\nobvious repetition and generic copy. I‚Äôd already written the PRD\nand requirements; this year was about shipping the experiment\nsafely and proving impact.\nBecause rollback needed to be painless, we didn‚Äôt overwrite the\nlegacy content or events. Instead, I worked with QA and\nengineering to run a closed, A/B experiment. In Mixpanel, rather\nthan creating a new event schema, I reused the existing\nastrology shortvibe events and added a tag/field to distinguish\nvariants (legacy astrology vs. new shortvibes). This kept tech\neffort low and made dashboards immediately comparable.\nExecution surfaced some early issues. For example, the new\nshortvibes displayed correctly in the US but not in India; even\nthough India wasn‚Äôt our primary market, I pushed for global\nparity‚Äîif the app is live, the experience should be consistent\neverywhere. We also identified mismatches where the wrong\nshortvibe type was being served (e.g., a media recommendation\nshowing up where a first date idea should have been). I flagged\nand resolved these before rollout.\nTo generate content at scale without blocking on engineering, I\nbuilt lightweight automations:\n‚óè Google Sheets + App Script to ingest poll questions/options\nand generate one-liners row-by-row.\n‚óè A parallel flow in Make.com so non-technical teammates\ncould operate it.\nOnce the system was stable, we rolled out the experiment and\nclosely monitored Mixpanel to track adoption, spot anomalies\nearly, and be prepared to act quickly if anything went wrong.\nThe results were strong:","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"8-8","partInfo":"Part 8 of 34"},"chunkIndex":7,"boundaries":{"startChar":14678,"endChar":16773,"overlapWords":22,"splitAt":"paragraph_break"},"tokenCount":419},{"text":"stable, we rolled out the experiment and closely monitored Mixpanel to track adoption, spot anomalies early, and be prepared to act quickly if anything went wrong. The results were strong:\n\n‚óè View ‚Üí Wave (seeing a shortvibe and then sending a wave)\nincreased ~50% vs. astrology shortvibes.\n‚óè Users sent ~45% more waves overall.\n‚óè Within the experiment cohort, the new shortvibe types drove\n~60% of waves, while the legacy ‚Äúcommon-options‚Äù\nshortvibes‚Äô share dropped from ~50% to ~40%.\nBeyond the numbers, we saw qualitative impact: in early match\nconversations, users began directly referencing our shortvibes\n(e.g., continuing a ‚Äúcoffee date‚Äù suggestion or chatting about a\nrecommended show). This proved the new shortvibes weren‚Äôt just\nincreasing clicks‚Äîthey were actively helping users break the ice\nand start more natural conversations.\nWith evidence in hand, we pushed toward full rollout. One\nremaining question was debate topics. I was cautious: generic\ndebates risk irrelevance, and ‚Äúdebate energy‚Äù depends heavily on\npersonal interests. Still, to test the boundaries I:\n1. Prototyped a common-option ‚Üí debate topic generator (App\nScript / Make.com + GPT).\n2. Produced ~100 topics across ~25‚Äì30 questions.\n3. Reviewed outputs with the team.\nAs expected, the common-option approach wasn‚Äôt good enough‚Äîtoo\nmany mismatches. We pivoted to a curated repository of broadly\nappealing, US-relevant debate prompts, tuned to be specific\nenough to be engaging but general enough to avoid dead ends. In\nparallel, I refreshed the media catalogs (top creators on\nYouTube, trending movies & shows on Netflix/Prime, current top\nSpotify artists), then regenerated recommendations via our\nsystem prompts. First date ideas got a larger, more diverse set\nas well.\nAfter QA, we handed the full package to product/engineering:\ntemplates, prompts, repositories, and the operating plan to\nreplace astrology shortvibes with this new IP. I also shared a","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"9-9","partInfo":"Part 9 of 34"},"chunkIndex":8,"boundaries":{"startChar":0,"endChar":1933,"overlapWords":30,"splitAt":"paragraph_break"},"tokenCount":384},{"text":"After QA, we handed the full package to product/engineering:\ntemplates, prompts, repositories, and the operating plan to\nreplace astrology shortvibes with this new IP I also shared a\n\nscaling roadmap‚Äîhow to personalize further by folding in\ndata-science signals once the base experience stabilized.\nEnd-to-end, this work moved us from generic, repetitive\nastrology snippets to actionable, conversation-starting\nshortvibes that users actually used‚Äîmeasurably improving\nengagement while keeping implementation lightweight for the\nteam.\nVoice-Based UXR Agent\nBy this point, I had built a strong foundation in creating\nvoice-based agents. I had already experimented with onboarding\nflows and matchmaking assistants, so I understood the nuances of\ndesigning conversational systems. The next step was applying\nthis knowledge to help the UXR (User Experience Research) team.\nThe challenge was clear: our UXR team had just one researcher\nbased in India, while most of our users were in the US. Time\nzone differences and accent gaps limited the number and quality\nof interviews. Even after attempts to hire in the US, costs were\ntoo high and didn‚Äôt scale. At best, we could conduct two user\ncalls a day, but scaling to 50‚Äì100 calls daily for richer\ninsights was impossible with manual work. Each call wasn‚Äôt just\nthe conversation‚Äîit required preparation, follow-ups,\ntranscripts, summaries, and insight documentation.\nSo, I designed a voice-based UXR agent to automate the first\nlayer: conducting the calls, asking pre-defined research\nquestions, handling follow-ups, and summarizing the insights. To\nbuild it, I first worked closely with the UXR researcher to\nunderstand her scripts‚Äîwhat she asked, how she followed up, and\nthe structure of conversations.\nI then wrote a system prompt to define the agent‚Äôs personality\nand guardrails. It was instructed to gracefully handle unknowns\n(e.g., ‚ÄúI‚Äôll get back to you‚Äù or escalating to a human\nresearcher) rather than hallucinate. I also fed it reference\nmaterial‚ÄîPRDs, feature descriptions, and past user\ntranscripts‚Äîso it could respond knowledgeably about product\nissues. To keep conversations natural, I designed for small\nhuman-like touches: pauses, slower pacing, acknowledging\ninterruptions, and filler words. Since ElevenLabs‚Äô most\nhumanized voices weren‚Äôt out yet (with breathing and laughter","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"10-10","partInfo":"Part 10 of 34"},"chunkIndex":9,"boundaries":{"startChar":0,"endChar":2335,"overlapWords":1,"splitAt":"paragraph_break"},"tokenCount":448},{"text":"Since ElevenLabs‚Äô most\nhumanized voices weren‚Äôt out yet (with breathing and laughter\n\nsupport), I had to mimic these behaviors manually through\ncareful scripting.\nOn top of that, the agent provided sentiment tagging at the end\nof each call. If the interaction was positive, it could even\nnudge the user to leave a Play Store/App Store rating‚Äîturning\nresearch into an opportunity for organic growth.\nI iterated multiple times, testing myself and with internal\nteammates. Feedback guided refinements until the agent felt\nnatural and usable. Once stable, I documented the system\nprompts, logic, and flows, then handed it off for integration.\nBehind the scenes ElevenLabs and Twilio were used and the\nworkflow would trigger when users could either schedule a slot\nor select to speak on the spot to the UXR Agent within the app.\nInitial campaigns were intentionally narrow‚Äîwe didn‚Äôt want scale\nyet, we wanted signal quality. The first ~10 calls showed\npromising results: transcripts and summaries flowed into the UXR\nteam, sentiment analysis was accurate, and the researcher could\nstill review recordings to validate findings. The bottleneck of\none human researcher doing all the calls was broken.\nThis agent solved three critical problems at once:\n1. Scalability ‚Äì made it realistic to target dozens of calls a\nday.\n2. Cost ‚Äì avoided hiring expensive US-based researchers for\nbasic interviews.\n3. Quality ‚Äì supported multiple accents, provided consistent\nquestioning, and freed humans to focus on deeper analysis\nrather than logistics.\nFor me, this project was the culmination of months of agent\nexperimentation‚Äîturning scattered learnings about prompts,\naccents, UX, and automation into a production-ready research\ntool that directly improved our user understanding.\nHunch IP: Onboarding to Date ‚Üí Web Onboarding and Matching\non App","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"11-11","partInfo":"Part 11 of 34"},"chunkIndex":10,"boundaries":{"startChar":20589,"endChar":22418,"overlapWords":12,"splitAt":"paragraph_break"},"tokenCount":367},{"text":"Hunch IP: Onboarding to Date ‚Üí Web Onboarding and Matching\non App\n\nThis initiative extended from work we had started in FY 24‚Äì25,\nwhere I first experimented with voice-based onboarding flows. At\nthat time, we wanted to test whether voice agents could handle\nthe entire onboarding journey: asking MBTI questions, collecting\nuser details, and completing verification. While the voice\nprototype worked, we quickly discovered a scale problem: what\ntook 5‚Äì10 minutes in text onboarding stretched to 30 minutes in\nvoice, even after optimizations. Breaking it into smaller calls\nrisked heavy drop-offs, so we reconsidered the approach.\nThe breakthrough was to shift from voice onboarding to a\nfrictionless web onboarding flow. Users didn‚Äôt need to download\nthe app or sign up first. Instead, they could answer MBTI\nquestions and basic details directly on a lightweight webpage.\nAt the end, they received:\n‚óè Their MBTI type and insights.\n‚óè A preview of potential matches (hooks).\n‚óè A prompt to unlock full access by purchasing a plan.\nOnly after payment were they asked to download the app. At that\npoint, their entire profile‚Äîanswers, details, MBTI tags‚Äîwas\nalready pre-filled from the backend. This flipped the model:\ninstead of nudging users multiple times after app install to go\npremium, we collected revenue upfront during onboarding.\nMy role as Associate PM was to support the Product Manager\nleading this IP. I handled planning, testing, and content\ncreation:\n‚óè Profile Curation: I built a pipeline for images shown on\nthe website. Using ImageGen for AI-generated photos and\nreal user data, I applied our attractiveness score logic\n(clarity of face, good lighting/background, no low-quality\nshots). I combined this with activity filters (active in\nlast 30 days) and match rate signals. Then, I manually\nreviewed top profiles to validate system accuracy and\nensure diversity across four age brackets (18‚Äì24, 24‚Äì32,\n32‚Äì42, 42+).","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"12-12","partInfo":"Part 12 of 34"},"chunkIndex":11,"boundaries":{"startChar":22353,"endChar":24279,"overlapWords":12,"splitAt":"paragraph_break"},"tokenCount":394},{"text":"Then, I manually\nreviewed top profiles to validate system accuracy and\nensure diversity across four age brackets (18‚Äì24, 24‚Äì32,\n32‚Äì42, 42+).\n\n‚óè Enrichment: For AI-generated profiles, I automated name and\nage assignment using AI inference. I also generated\ntestimonials by transcribing UGC ads from creators, feeding\nthem into GPT, and converting them into written\nendorsements that matched the tone of the videos. These\nwere paired with embedded videos to create credible\ntestimonial sections.\n‚óè Implementation Support: I collaborated with developers to\nembed testimonials correctly (ensuring the right video\nmapped to the right text) and to resolve issues with\nmissing or slow-loading images. To fix oversized images, I\nset up a local lossless compression workflow (via\nGPT-assisted scripts) that reduced all files below 1 MB\nwithout losing quality, then re-uploaded them to S3 with\nconsistent naming.\n‚óè Events & QA: I did extensive event testing. Bugs included\nMBTI logic not firing correctly, button malfunctions, and\nmissing or incomplete data in triggered events. After every\nsmall fix, I re-ran the full user journey (onboarding ‚Üí\npayment ‚Üí profile creation) to confirm stability. This\nrequired creating dozens of new test accounts with fresh\nemails.\nThe feature went live and immediately created a new revenue\nstream. Out of ~$5‚Äì6k monthly revenue, ~$1‚Äì1.5k came directly\nfrom the web onboarding flow. While modest at launch, it was\nsignificant proof of a scalable model: conversion rates were\nhigher because users experienced value first (MBTI tag + preview\nof matches), then were asked to pay.\nThis was Version 1, but it established a strong foundation:\n‚óè Reduced onboarding friction.\n‚óè Created a direct revenue pathway tied to first-time\nonboarding.\n‚óè Validated that blending AI-generated + real user content\ncould power a convincing experience.","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"13-13","partInfo":"Part 13 of 34"},"chunkIndex":12,"boundaries":{"startChar":24139,"endChar":25994,"overlapWords":21,"splitAt":"paragraph_break"},"tokenCount":371},{"text":"‚óè Validated that blending AI-generated + real user content\ncould power a convincing experience.\n\nBy the time I left, both I and my manager had moved on, so I‚Äôm\nunsure how it evolved further. But even at this stage, it had\nthe potential to become a major revenue driver for Hunch by\nshifting the economics of premium subscriptions from reactive\nnudges inside the app to proactive purchase during onboarding.\nFinetuning the Chat Model\nThis was one of my largest and most important projects at Hunch.\nThe goal: keep users engaged longer by making our in-app\nconversations feel human. We already had a live chat-bot layer\nbuilt on a vanilla GPT model, but it suffered obvious flaws:\n‚óè It frequently forgot context.\n‚óè Replies were over-optimized and robotic.\n‚óè In a dating setting, users quickly realized they were\ntalking to AI.\nMy job was to solve those issues and propose a better version\nthrough data-driven fine-tuning.\nI began by pulling real user chats. A co-worker‚Äôs old SQL script\nsupposedly did this, but it captured only one-sided messages (my\n‚Äúhi‚Äù was saved, your ‚Äúhello‚Äù wasn‚Äôt). I rewrote the query so\neach row contained the full two-way exchange, limited the sample\nto the past three months, and included only threads with at\nleast five messages.\nTo avoid mid-thread snippets, I required that at least one\naccount was created within that three-month window, ensuring the\nchat started‚Äîand often ended‚Äîinside our slice. After several\nGPT-assisted iterations I finally had complete two-way data.\nBefore anything else, I scrubbed the logs. Real names became\nUser 1 and User 2; phone numbers, handles, and other identifiers\nwere replaced with dummies unless I explicitly needed them\nlater. I did the first pass by hand, then wrote an Apps Script\nto validate & catch anything I missed.\nNext I built a rating pass using GPT as a judge (with a long\nsystem prompt) to score each conversation on:","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"14-14","partInfo":"Part 14 of 34"},"chunkIndex":13,"boundaries":{"startChar":25899,"endChar":27795,"overlapWords":14,"splitAt":"paragraph_break"},"tokenCount":414},{"text":"Next I built a rating pass using GPT as a judge (with a long\nsystem prompt) to score each conversation on:\n\n‚óè Depth & progression (do they get beyond small talk; do they\nangle toward a date?)\n‚óè Flirtation & emotional tone (playful, supportive,\nteasing‚Äîwithout being creepy)\n‚óè Naturalness (slang, shortforms like ‚Äúu / ur / ttyl‚Äù,\nemojis)\n‚óè Healthy back-and-forth (no walls of text; real turn-taking)\n‚óè Signals of action (exchanging details; discussing plans;\nmoving off-app‚Äîthough our bots would later learn to deflect\nthat)\nAlso told GPT to explain its score and highlight which\nconversations should be included in our training.\nEarly runs penalized consensual ‚Äúexplicit‚Äù chats too harshly, so\nI revised the rubric: adult content is fine if the user\ninitiates it. That kept things honest to how people really flirt\nand talk on dating apps.\nAfter several cycles I hand-audited the top-rated threads,\nmerged any GPT oversights, and ended with ~200 high-quality\ntwo-way conversations (‚âà400 messages) for the first model.\nFurther I created ~25 high-quality two-way synthetic\nconversations (‚âà50 messages) only where we lacked coverage,\nmaking the entire total to ~225 high-quality two-way\nconversations(‚âà450 messages).\nWith that baseline in place, I went for data breadth rather than\njumping straight into a model run. We‚Äôd ship a general bot first\n(nothing specific like separate for male and female users), but\nI wanted to test a hypothesis: would gendered assistants (one\nthat ‚Äúspeaks like a woman,‚Äù one that ‚Äúspeaks like a man‚Äù) add\nany realism? So alongside the general dataset, I prepared two\npersona sets where the assistant persona was explicitly male or\nfemale. That meant re-threading conversations so the assistant‚Äôs\nlines aligned with the intended persona, swapping user/assistant\nroles where needed to preserve a natural user‚Üíassistant turn\norder, and dropping threads that didn‚Äôt make sense for that\npersona. I did this before our first training so we wouldn‚Äôt\nhave to re-harvest later if the idea proved useful.","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"15-15","partInfo":"Part 15 of 34"},"chunkIndex":14,"boundaries":{"startChar":27689,"endChar":29710,"overlapWords":21,"splitAt":"paragraph_break"},"tokenCount":415},{"text":"I did this before our first training so we wouldn‚Äôt\nhave to re-harvest later if the idea proved useful.\n\nNext came formatting. I started by writing an AppScript that\nturned the cleaned sheets into trainable JSON with strict\nrole/content pairs in the order user ‚Üí assistant. I kicked\noff a first run in the OpenAI Playground (kept parameters\nbasically default to save time‚Äîno fancy tuning, temperature\nleft at default). That training failed. My exporter had let\na few malformed rows slip through; I had checked manually,\nbut a couple of ‚Äúuser‚Üíuser‚Äù or ‚Äúassistant‚Üíassistant‚Äù pairs\nwere hiding in the long tail. Only then did I write an\nAppScript to scan every item and flag lines that broke the\nstructure. With those fixed, the training completed cleanly\nin the Playground.\nI fine-tuned the general model first; the male/female variants\ncame later. Within 30 minutes each, I had three custom\ncheckpoints ready to test.\nWith a working checkpoint in hand, I turned to the prompt that\nwould steer it. I wrote a long system prompt that encoded the\nrules of dating conversation:\n‚óè Be flirty when appropriate; small grammar imperfections are\nokay.\n‚óè Never admit you‚Äôre a bot; stay in character.\n‚óè Allowed mild typos and usage of emojis and slangs so\nreplies felt human.\n‚óè Respect boundaries: the bot does not initiate explicit\ntopics; if the user does it participates in it.\nNow that everything was in place I created a QA checklist so I\ndon‚Äôt miss at least the important cases to look for and share\nwith my co-workers who were also testing this.\nBelow are few things we found not working:\n‚óè Flirting skills & Response to Adult content‚Äì The bot‚Äôs\nresponses weren‚Äôt as playful as needed. It sometimes","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"16-16","partInfo":"Part 16 of 34"},"chunkIndex":15,"boundaries":{"startChar":29607,"endChar":31298,"overlapWords":19,"splitAt":"paragraph_break"},"tokenCount":376},{"text":"Below are few things we found not working:\n‚óè Flirting skills & Response to Adult content‚Äì The bot‚Äôs\nresponses weren‚Äôt as playful as needed It sometimes\n\nrefrained from participating in adult talks.\n‚óè Emoji / slang usage ‚Äì It recognized emojis but used them\nsparingly. Slang was almost absent.\n‚óè Language handling ‚Äì It responded to Hindi, Gujarati, even\nChinese, when in reality no human user on a dating app\nwould speak 50 languages fluently. This broke immersion.\n‚óè Time awareness ‚Äì It had no sense of whether it was morning\nor night, so replies sometimes clashed with reality.\nI logged all of this so that when we do the next training all of\nthese issues are resolved.\nThe male assistant and female assistant variations performed no\nbetter than the general one. In hindsight that made sense: their\ndata wasn‚Äôt truly distinct‚Äîwe‚Äôd remapped from the same pool. So\nparked the idea for a future revisit and doubled down on the\ngeneral model.\nFor the second pass, I made a structural change: every training\nmessage bundle started with a system role. That meant our\ndataset had to become system ‚Üí user ‚Üí assistant for every\ntraining example, not just a single global system instruction.\nThe tech lead also pushed for a shorter system prompt (lower\ntokens = lower cost), and suggested we embed identity per\nconversation (names/ages/locations) in the system message so the\nbot wouldn‚Äôt forget who it was and whom it is speaking to. Doing\nthis properly meant reworking the whole data path.\nI wrote a new SQL query to pull the fields we needed to\nparameterize each system prompt: (initiator/respondent) names,\nages, and locations. Because our earlier filters had already\nnarrowed conversations, I mapped on conversation_id and then\ninferred who sent the first message to determine ‚Äúuser‚Äù vs\n‚Äúassistant‚Äù in our fine-tuning pairs. Where accounts had been\ndeleted or fields were missing, I generated consistent,","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"17-17","partInfo":"Part 17 of 34"},"chunkIndex":16,"boundaries":{"startChar":0,"endChar":1899,"overlapWords":1,"splitAt":"paragraph_break"},"tokenCount":411},{"text":"Where accounts had been\ndeleted or fields were missing, I generated consistent,\n\nfictional replacements for age, location, and name.\nAs raw names should never be used directly. I shuffled male\nnames only among males and female names only among females to\npreserve gendered cues without leaking identities. If a chat\nmentioned ‚ÄúI live in X‚Äù or stated an age, I verified that the\nsystem prompt variables matched the message content. When they\ndidn‚Äôt, I updated the synthetic persona values or scrubbed the\nmessage. Handles (Instagram/Snapchat) and phone numbers were\nreplaced with dummy tokens. This was manual first (to avoid\nmisses).\nI converted our long instructions into a compact system\nparagraph that carried all the rules we needed: flirty/casual\ntone, emojis/slang allowed, English-only (politely ask to switch\nto English if needed), never admit being a bot, okay to continue\nadult topics only if the user initiates, and keep things\nrealistic (not overly perfect). On top of those rules, each\nsystem message injected the persona variables (Name, Age, City)\nso the assistant wouldn‚Äôt drift when history got truncated\nduring deployment.\nFrom the first pass we knew flirting, slang, and emoji rhythm\nneeded more coverage. I added high-quality real snippets where\navailable and supplemented with synthetic but human-sounding\nconversations that demonstrated natural flirting, contemporary\nshorthand (‚Äúwyd‚Äù, ‚Äúfr‚Äù, ‚Äúüò≠‚Äù), and subtle grammar quirks. Add a\nfew Adult conversations too so that it can now respond and\nco-operate whenever the user initiates it.\nI rewrote my Apps Script that generates the training JSON:\n‚óè Prepend role: \"system\" with the templated prompt for every\nexample.\n‚óè Then user and assistant turns.","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"18-18","partInfo":"Part 18 of 34"},"chunkIndex":17,"boundaries":{"startChar":32968,"endChar":34685,"overlapWords":12,"splitAt":"paragraph_break"},"tokenCount":345},{"text":"I rewrote my Apps Script that generates the training JSON:\n‚óè Prepend role: \"system\" with the templated prompt for every\nexample ‚óè Then user and assistant turns.\n\n‚óè Ensure strict alternation (system ‚Üí user ‚Üí assistant) and\nmaintain message pairing.\nI also rewrote AppScript which checked user ‚Üí assistant pairs. I\nextended it to verify each example begins with a system role and\nthat every trio is complete. On failure it returned\nline-accurate errors so I could fix the exact rows instead of\nscanning the whole file by eye.\nWith this rebuilt pipeline, I exported the new training set and\nran the fine-tuning again on Playground with defaults for\ntraining parameters. The Playground test runs now reflected the\nper-message system instruction, so the model‚Äôs tone and\nboundaries were much more consistent across examples.\nAlong the way to understand cost vs. quality, I trained the one\nwith a long system prompt variant too but this time with the\npersona variables (Name, Age, City) injected on the top so the\nassistant wouldn‚Äôt drift when history got truncated during\ndeployment. In the Playground, both short and long system\nprompts behaved properly because the full context was present.\nThe long variant was naturally richer‚Äîmore nuance, better humor.\nThe short variant held its own: flirting improved, emojis and\nslang showed up, co-operated in adult talks and it reliably\nredirected other languages back to English. So far, so good.\nI invited other co-workers, my manager and folks from the tech\nand product team to test it out and they were quite satisfied\nwith this one. So we went ahead and got it integrated on the app\nin the dev environment to further test if things are working\nproperly there or not.\nWhen we integrated the fine-tuned bot into the app, we\nencountered several challenges. Some of these were expected and\nplanned for, while others surfaced only after live testing.\nBelow is a breakdown of the key issues:\n1. Lack of Time Awareness\nThe bot was not aware of the current time context (morning,\nafternoon, evening). As a result:","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"19-19","partInfo":"Part 19 of 34"},"chunkIndex":18,"boundaries":{"startChar":0,"endChar":2047,"overlapWords":1,"splitAt":"paragraph_break"},"tokenCount":445},{"text":"Lack of Time Awareness\nThe bot was not aware of the current time context (morning,\nafternoon, evening) As a result:\n\n‚óè It sometimes generated replies that didn‚Äôt align with\nreality, e.g., ‚ÄúLook at the sunset, how beautiful it is‚Äù in\nthe early morning.\n‚óè In other cases, it would say ‚ÄúI‚Äôm going to sleep. Good\nnight‚Äù even when the user was chatting in the afternoon.\nThis mismatch reduced the naturalness and believability of\nconversations.\n2. Unrealistic Response Speed\nThe bot responded almost instantly, which felt unnatural\ncompared to human behavior.\n‚óè Human conversations usually have natural pauses or delays.\n‚óè Without these delays, the interaction clearly felt like\n‚Äúchatting with a bot,‚Äù breaking immersion.\n3. Risk of Over-Engagement with Bots\nBecause the conversations were engaging and responsive, there is\na risk that:\n‚óè Users might prefer spending time chatting with bots instead\nof engaging with real users.\n‚óè This goes against our core objective: the bot should\nenhance the environment, not replace genuine user-to-user\ninteraction.\nThe bot should act as an engagement booster, especially for\nonboarding and habit-formation, but not dominate the user‚Äôs\ntime.\n4. Limited Awareness of Current Events","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"20-20","partInfo":"Part 20 of 34"},"chunkIndex":19,"boundaries":{"startChar":0,"endChar":1212,"overlapWords":1,"splitAt":"paragraph_break"},"tokenCount":247},{"text":"4 Limited Awareness of Current Events\n\nThe bot lacked awareness of recent or real-world events. For\nexample, it could not reference:\n‚óè What happened today or yesterday.\n‚óè What‚Äôs happening tomorrow.\nWhile this could make conversations feel outdated or generic, we\ndecided not to prioritize solving this in version 1, since\nenabling live web search would significantly increase costs. For\nnow, this limitation is acceptable, and we can revisit it in\nlater versions if needed.\n5. Forgetting Conversation Context\nOn the OpenAI Playground the entire conversation history is\nautomatically preserved in one chat session, but as we set up\nthe finetuned model on our app it required us to supply\nconversation history for the bot to maintain context. This\ncreated several challenges:\n‚óè For example, if a user mentioned their occupation as ‚ÄúCA in\nXYZ company‚Äù ten messages earlier, the bot might later\ncontradict itself and say it works as a waiter or gardener.\n‚óè While basic details like name, age, and location could be\nset in the system prompt, it wasn‚Äôt feasible to hardcode\nevery possible fact since conversations can go in endless\ndirections.\n6. Handling Threaded Replies\nAnother challenge arose when users replied to older messages\nwithin a thread rather than continuing linearly in the chat. For\nexample:\n‚óè The user initially asked: ‚ÄúWhat do you think about Trump as\na president?‚Äù","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"21-21","partInfo":"Part 21 of 34"},"chunkIndex":20,"boundaries":{"startChar":0,"endChar":1376,"overlapWords":1,"splitAt":"paragraph_break"},"tokenCount":292},{"text":"For\nexample:\n‚óè The user initially asked: ‚ÄúWhat do you think about Trump as\na president?‚Äù\n\n‚óè The conversation then shifted in another direction.\n‚óè Later, the user returned to that earlier message with a\nreply like: ‚ÄúWhat about this one now?‚Äù\nIf the bot only received the latest message (‚ÄúWhat about this\none now?‚Äù) without the original reference, it would generate a\nvague or irrelevant response. The reply needed to be grounded in\nthe original base message to maintain coherence.\nAfter discussions around possible solutions below are few\nsolutions that we implemented to solve the above issues:\n‚óè Current time injection: We attached the server‚Äôs current\ntimestamp to the system context so the assistant could say\n‚Äúgood morning/evening‚Äù appropriately and avoid\ntime-clashing phrases.\n‚óè Reply delays: We set a random 1‚Äì10 minute delay before\nsending a response, to avoid ‚Äúinsta-reply‚Äù bot vibes. Later\nwe planned a smart tweak: if the user went offline, we\ncould trigger the reply earlier to nudge them back with a\nnotification.\n‚óè Ghosting cap: To avoid users endlessly chatting with bots,\nwe capped bot replies at 15‚Äì20 in v1. After that, the bot\nwould stop responding. This ensured people still had ample\nhuman matches and nudged them back to real conversations.\n‚óè Message History: Initially, we decided to maintain the last\n10 messages as context. However, it became clear that this\nmeant 5 user messages + 5 bot messages only, which was\ninsufficient. To improve continuity, we expanded the window\nto 20 messages (10 from the user, 10 from the bot). This\nensured the bot remembered more of the conversation and\nreduced inconsistencies in persona or details.\n‚óè Context for Reply in Thread: We updated the logic so that\nwhenever a message is identified as a reply in a thread,\nthe system passes the base/original message too in the\nmessage history. By combining them, the bot could generate\na relevant and accurate response tied to the intended\ncontext.","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"22-22","partInfo":"Part 22 of 34"},"chunkIndex":21,"boundaries":{"startChar":38923,"endChar":40875,"overlapWords":16,"splitAt":"paragraph_break"},"tokenCount":422},{"text":"By combining them, the bot could generate\na relevant and accurate response tied to the intended\ncontext.\n\nFinally when everything seemed to work as expected we asked the\nfounders to test and they said the chat had many issues and was\nnowhere close to what we had described.\nThis was odd as we all had tested it as I set with the developer\nto understand what‚Äôs going on in the code and asked to see the\nsystem prompts, the version of the fine-tuned chatbot version we\nare using, etc to see if anything of them isn‚Äôt from the old\nexperiments. And yes it was the same case wherein when he was\ncleaning up a few things in code he had mistakenly changed the\nsystem prompt to an older one thinking that‚Äôs the latest one.\nAs this was solved we again asked the founders to test and they\nwere quite happy so we were ready to launch this new chatbot.\nI created Mixpanel Boards to track things and also wrote SQL\nqueries to compare how many messages users used to send before\nthe new chatbot and what‚Äôs that number after the release of this\nchatbot.\nThe numbers were quite shocking and how good the bot had become\nnow was something everyone in the company noticed.\nFew of the best results were:\n‚óè Average messages per bot chat jumped from ~5 to ~15.\n‚óè Session time length doubled (~100 %).\n‚óè UXR calls captured comments like ‚ÄúPeople here actually chat\nback; conversations flow.‚Äù Users didn‚Äôt realize most of\nthose ‚Äúpeople‚Äù were bots.\nSomewhere in the middle of this I also realized two things.\nFirst, that careful data curation, privacy discipline, AppScript\nplumbing, and ruthless iteration can move a product a long way\nwithout a huge team. Second, I wanted to go further than this\nproject allowed: agents, MCP patterns, front- and back-end\nwiring, image/video generation at production quality‚Äîthe full\n0‚Üí1‚Üí10 path. My health had taken a hit over the past couple of\nyears; most weekends were spent at home learning, testing, or on\nwebinars. The founders and my manager asked me to stay‚Äîoffered\nto raise pay and sponsor courses‚Äîbut I knew if I tried to ‚Äúdo\nboth,‚Äù I‚Äôd do neither well. I wanted a clean block of time to","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"23-23","partInfo":"Part 23 of 34"},"chunkIndex":22,"boundaries":{"startChar":40771,"endChar":42879,"overlapWords":17,"splitAt":"paragraph_break"},"tokenCount":491},{"text":"asked me to stay‚Äîoffered to raise pay and sponsor courses‚Äîbut I knew if I tried to ‚Äúdo both,‚Äù I‚Äôd do neither well. I wanted a clean block of time to\n\nlearn deeply, reset priorities, and recover a bit. The decision\nwasn‚Äôt reactive; it was a sum of things: wanting broader\nownership, wanting recognition to match scope, and wanting to\ninvest in skills I could compound for years. So with a heavy\nheart I finally went ahead with resignation to study AI\nfull-time.\nPerfect‚Äîthanks for the nudge. I‚Äôve fixed the ‚Äúvalidation at the\nend‚Äù bit (you verified SQL against raw numbers as you built each\ntable, not just in the last days), and I‚Äôve expanded the ‚Äúhow I\ndebugged/learned/solved it‚Äù moments with concrete, step-by-step\ndetail. Kept your narration and paragraph flow; only tightened\ngrammar and clarity and added the deeper troubleshooting detail\nyou asked for.\nRetool Marketing Dashboard\nOkay, so we started the Retool dashboard project‚Äîthis was the\nlast piece of work I did for the company. The toughest part was\nthat I had to work directly with the founders, and the use case\nwas critical. We needed to monitor various third-party websites\nlike Shortimize. Tools like that let you create analytics\nreports for social media accounts and track all your videos\nacross TikTok, Instagram, and YouTube. Once you add videos to a\ntracking sheet, the tool fetches updated numbers every few hours\nand gives you dashboard views by channel or video: what‚Äôs\ntrending, when the last video was uploaded,\nviews/likes/comments, how a specific video performs relative to\nchannel averages, and so on.\nYou get deep overviews for each channel and each video, but\nwhat‚Äôs missing in these third-party solutions is sentiment:\nthere‚Äôs no built-in analysis of what users are commenting, how\noften the brand is mentioned, or whether particular creators are\ntalking about you in the comments. We needed a custom, internal\ndashboard that could: (1) catch viral videos very early, (2)\nanalyze comment sentiment, (3) track how often our app or\nhandles are mentioned, and (4) support creator workflows (e.g.,\nif creators are contracted to engage in comments and mention the\napp). Building this in-house would save thousands of dollars\n(Shortimize alone cost us ~$200/month, and we still needed 2‚Äì3","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"24-24","partInfo":"Part 24 of 34"},"chunkIndex":23,"boundaries":{"startChar":0,"endChar":2265,"overlapWords":30,"splitAt":"paragraph_break"},"tokenCount":483},{"text":"Building this in-house would save thousands of dollars\n(Shortimize alone cost us ~$200/month, and we still needed 2‚Äì3\n\nmore tools) and could generate outsized marketing value. This\nbecame my project in my last weeks at Hunch.\nHonestly, I didn‚Äôt feel much energy at the start‚ÄîI had just\nspent 8‚Äì9 days in the hospital. Coming out and jumping straight\ninto a big, founder-facing project wasn‚Äôt easy. We split\nresponsibilities: another teammate owned Apify APIs, building\nthe data pipeline (cleaning, storing, and automating updates\nevery few hours into Redshift). I owned the Retool app: building\na dynamic dashboard, wiring Redshift to Retool, writing SQL for\nall views, and handling the integrations. Since it came directly\nfrom the founders, I took it on and began.\nFirst, I analyzed how Shortimize works because we wanted to\nbroadly replicate it. I reviewed the dashboards we had set up\nthere: tables, graphs, channel-level overviews, video-level\ndetail, and per-video deep dives. We‚Äôd also built ‚Äúcollections,‚Äù\nwhere each collection contains multiple accounts with its own\noverview and video set. Like any analysis platform, you get\nfilters (last 7 days, by account, by video ID/title, etc.). I\ntook screenshots of the Shortimize dashboards, shared links with\nGPT, and explained on-click behaviors and page transitions so it\ncould grasp the UX. The goal: recreate something similar in\nRetool.\nI had never used Retool before, so I made a free account,\nlearned what‚Äôs included vs. paid (found that we didn‚Äôt need the\npaid version for now), and started studying. The big challenge:\nthere weren‚Äôt many good video tutorials, and most resources were\ntext-heavy docs‚Äînot ideal when you need to see components in\naction. Still, I wanted to understand Retool before building.\nWithout that, I‚Äôd be flailing.\nAt one point GPT suggested generating a Retool JSON I could\nimport to scaffold pages instantly. Since we didn‚Äôt have data\nyet (the data engineer was still wiring Redshift), I tried\nsupplying dummy data. Even then, imports kept failing with\ndifferent errors. I spent ~1 day trying, with long GPT threads,\nbut nothing stuck. If that had worked, it would have saved a\nweek. It didn‚Äôt‚Äîso I switched back to fundamentals: learn Retool\nthe normal way.\nI found a handful of useful (though mostly year-old) YouTube\nvideos and read some docs. On day three, my founder and I paired\nfor ~6 hours. We built a side panel, a table, connected Retool","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"25-25","partInfo":"Part 25 of 34"},"chunkIndex":24,"boundaries":{"startChar":44880,"endChar":47315,"overlapWords":18,"splitAt":"paragraph_break"},"tokenCount":515},{"text":"On day three, my founder and I paired\nfor ~6 hours We built a side panel, a table, connected Retool\n\nto Redshift, tested the connection, set permissions, and finally\nsaw real data in the dashboard. I learned about local storage,\nactions, where to attach queries, and how to bind SQL outputs to\ntables. From then on we had daily calls: initially 2‚Äì3 hours,\nthen ~1 hour each evening to sync on progress, blockers, and\nnext steps.\nIt quickly became obvious this wasn‚Äôt a one-week project. We had\nto link many moving parts and ensure data correctness. Writing\nSQL is easy when it‚Äôs simple‚Äîbut global filters complicate\neverything. If you pick ‚Äúlast 7 days,‚Äù every table and chart\nmust reflect it. If a table depends on another, filters must\ncascade correctly. We also needed to align on definitions (e.g.,\nengagement rate, virality) and decide whether to use\nShortimize‚Äôs formulas as-is or tweak them. Custom columns\nrequired mapping multiple source fields. And for comments, we\nhad no reference: we had to design the schema, decide how to\nvisualize, and define sentiment analysis outputs from scratch.\nAs with any custom build, the ‚Äúunknowns‚Äù reveal themselves only\nas you build. We extended the timeline to two weeks to do it\nright and backtest queries thoroughly.\nBecause video resources on Retool were limited, I bookmarked the\nbest few and avoided getting lost in docs. I started with the\nsimplest tab: Accounts. It just listed accounts within a\ncollection (total views, total comments, etc.). I shared\nscreenshots with GPT, explained click behaviors and custom\ncolumns, and asked for guidance on how to replicate on Retool.\nMy routine became: design UI components in Retool (drag/drop,\nhide/show conditions), then write SQL to power them. With GPT,\nI‚Äôd specify my tables, column names, and logic, but I kept\nownership of the logic path so I could build my own reasoning\nmuscle. If I was off, I asked GPT to correct me‚Äîand to comment\non the SQL thoroughly so I could learn from it.\nAnd here‚Äôs where I was very particular: whenever I created a\ntable or chart, I immediately validated its SQL result against\nraw numbers. I‚Äôd run the same logic on the Redshift SQL tab,\nsample a few accounts/videos, and cross-check totals and spot\nmetrics (views, likes, comments). Only after the numbers matched\ndid I move on. I repeated this pattern for every new component.\nThis habit saved me from carrying silent inaccuracies forward.","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"26-26","partInfo":"Part 26 of 34"},"chunkIndex":25,"boundaries":{"startChar":0,"endChar":2421,"overlapWords":1,"splitAt":"paragraph_break"},"tokenCount":528},{"text":"I repeated this pattern for every new component This habit saved me from carrying silent inaccuracies forward.\n\nWe also had to map weird field names from Apify APIs into normal\nconcepts. For example, a column called diggscount and\nheartscount both deal with likes wherein one is no. of likes\ngiven and another means no. of likes received. Because the data\ningestion follows its own conventions, we had to translate those\ninto human terms (views, likes, shares). That mapping fed every\ntab. For the Overview tab (the most complex), I needed playable\nvideos. Embedding TikTok videos was tricky‚Äîn‚Äôt working even\nafter many tries and then as I researched about it I figured out\nthat TikTok is quite strict in letting the videos playable\noutside it‚Äôs platform so I rather did a workaround: show the\nvideo cover image linked to the real video URL. Clicking opened\nthe video in a new tab. Not perfect, but good enough‚Äîand better\nthan just a text link. I also wanted numerical badges on tab\nbuttons (e.g., ‚ÄúVideos (100)‚Äù, ‚ÄúAccounts (8)‚Äù). My charts were\ncorrect, but wiring those counts into button labels was tricky.\nIt wasn‚Äôt critical for correctness, so I deprioritized it to\nfinish core charts and tables. Later, I circled back and\nimplemented it.\nMeanwhile, understanding how data was stored in Redshift was\ncrucial. Were updates overwriting old rows or appending? That\nchanges how you query. While cron jobs weren‚Äôt running, my SQL\nqueries worked fine. Once the every-two-hours pipeline started,\nresults drifted‚Äîbecause I needed to select only the latest\nsnapshot per video for dynamic metrics (views/likes/comments\nevolve) while treating account IDs and names as static. I\nupdated queries to pick the most recent record per video. I also\nmade sure every global filter (collections, accounts, specific\nvideos, time ranges) flowed all the way into each SQL‚Äôs WHERE\nand joins. Each time I changed logic, I manually re-validated\nthe numbers against raw data for a handful of test cases.\nAnother real-world wrinkle: cron jobs can fail or skip some\nvideos. That can make today‚Äôs totals appear lower than\nyesterday‚Äôs simply because a video‚Äôs new data didn‚Äôt arrive. I\nwrote down an operational rule of thumb for myself: whenever an\naggregate suddenly dips, first check pipeline completeness for\nthat interval before assuming a content trend. I flagged the\nneed for data validations and alerts, plus a small run-book: if\na video is missing, check if it was deleted, if the API call\nfailed, or if rate-limit hit.","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"27-27","partInfo":"Part 27 of 34"},"chunkIndex":26,"boundaries":{"startChar":0,"endChar":2501,"overlapWords":1,"splitAt":"paragraph_break"},"tokenCount":535},{"text":"the need for data validations and alerts, plus a small run-book: if a video is missing, check if it was deleted, if the API call failed, or if rate-limit hit.\n\nFor seven-day comparisons, there‚Äôs the off-by-one trap. To show\ndaily deltas properly, you need D-8 to compute the first delta\nin a 7-day window. So I fetched an extra day for calculations\nbut hid that in the display to keep the ‚ÄúLast 7 Days‚Äù honest. I\nlearned to always write that extra-day logic as a comment block\nright inside the SQL so future me (or someone else) wouldn‚Äôt\n‚Äúsimplify‚Äù it and break the math.\nThere were times I just couldn‚Äôt get something to reconcile. One\nexample: time filters. Retool and my local queries weren‚Äôt\ngetting the same output for exact same matching queries. I tried\nmultiple LLMs and asked the data engineer for help. We\neventually found Retool was offset by time zone (IST vs. UTC) by\n~5 hours. A tiny fix‚Äîhalf a day lost. The lesson: when things\ndon‚Äôt add up, check the simplest assumptions (time zones,\nlocales, data types) before diving deeper. From then on, I broke\nproblems into smaller pieces: first get UI visible, then wire\ndata, then confirm components match Shortimize behavior or find\nthe closest equivalent if Retool lacks something. For\ndaily/weekly toggle views, I initially used the quick path\n(hide/show separate charts) and later refactored into\nconditional logic inside a single chart based on button state.\nStart with a working baseline; optimize once it‚Äôs reliable.\nNow that V1 (Shortimize-style marketing dashboard) was largely\ncomplete it was time to work on V2 (comments section with\nanalysis scaffolding). A few columns were pending because I\nstill needed founder alignment on definitions. I explicitly\nflagged those: if we keep them, here‚Äôs what to decide; if not,\nwe can drop them.\nWhile most of the Retool work was about replicating Shortimize‚Äôs\nmetrics dashboards, the comments analysis tab was the one place\nwe weren‚Äôt just copying‚Äîwe were inventing. This was a feature\nShortimize didn‚Äôt offer, and it quickly became the most original\npart of the project.\nThe idea was simple: views and likes tell you how a video\nperforms, but comments tell you what people actually think. For\nHunch, that mattered. If users were mentioning ‚ÄúHunch‚Äù\npositively in comments, or comparing us against Tinder or\nBumble, that was gold for marketing. If users were negative or\nmocking, we needed to know early. So this dashboard was designed","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"28-28","partInfo":"Part 28 of 34"},"chunkIndex":27,"boundaries":{"startChar":0,"endChar":2442,"overlapWords":30,"splitAt":"paragraph_break"},"tokenCount":531},{"text":"If users were negative or\nmocking, we needed to know early So this dashboard was designed\n\nto give us visibility into the qualitative layer behind the\nnumbers.\nThe first problem was deciding what the comments dashboard\nshould even show. Apify gave us raw comment dumps, but they\nweren‚Äôt structured for insights. I worked with the founders to\ndefine a schema:\n‚óè Per video: Show all comments for a specific video (not\naggregated per account).\n‚óè Base fields: Comment text, commenter, likes on the comment,\ntimestamp.\n‚óè Replies: Captured as child rows, so a parent comment could\nexpand into a thread.\n‚óè Sentiment: Positive, neutral, negative ‚Äî placeholder until\nthe LLM pipeline was connected.\n‚óè Mentions: Count whenever ‚ÄúHunch,‚Äù ‚ÄúTinder,‚Äù ‚ÄúBumble,‚Äù or\n‚ÄúHinge‚Äù appeared in comments/replies.\nThat schema became the backbone. Everything else‚Äîfilters,\ncharts, sentiment summaries‚Äîbuilt on top of it.\nThe first working version was a plain table of comments per\nvideo. I connected it to Redshift with a SQL query and\ncross-checked it against raw JSON dumps to confirm counts\nmatched. Once stable, I added filters:\n‚óè Timeframe (last 7 days, last 14 days, last 30 days).\n‚óè Keyword search (find comments mentioning a word/brand).\n‚óè Top-N selector (show top 5, 10, 50, or all comments).\nThe Top-N filter was critical. Without it, pulling 500 comments\nin Retool slowed the UI to a crawl. By default, the table only\nloaded top 10 comments (ranked by likes), with a ‚Äúload more‚Äù\noption. This kept the dashboard usable.","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"29-29","partInfo":"Part 29 of 34"},"chunkIndex":28,"boundaries":{"startChar":0,"endChar":1500,"overlapWords":1,"splitAt":"paragraph_break"},"tokenCount":320},{"text":"By default, the table only\nloaded top 10 comments (ranked by likes), with a ‚Äúload more‚Äù\noption This kept the dashboard usable.\n\nReplies were tricky. Apify stored them in a slightly different\nstructure, and my first SQL joins were wrong. Some threads\nlooked empty when they weren‚Äôt. I manually picked a test video,\nopened its comments directly on TikTok, and compared. That‚Äôs how\nI discovered the parent‚Äìchild key mismatch. Once fixed, I built\na second linked table in Retool: whenever you clicked a comment\nrow, its replies appeared below in a child panel.\nThat small UX detail‚Äîexpandable replies‚Äîwas a first for us.\nShortimize never gave that granularity.\nI wasn‚Äôt responsible for wiring the Sentiment tagging directly,\nbut I drafted the integration plan:\n1. Batch comments per video.\n2. Send them to an LLM classifier for sentiment.\n3. Use regex for brand mentions.\n4. Save results back to Redshift with extra columns:\nsentiment, brand_mention.\nOn the Retool side, I created scaffolding that assumed those\ncolumns already existed:\n‚óè A pie chart of sentiment distribution per video.\n‚óè A counter widget: ‚ÄúHunch mentioned 42 times.‚Äù\n‚óè Filters: ‚Äúshow only negative comments,‚Äù or ‚Äúshow comments\nmentioning competitors.‚Äù\nDuring testing, I mocked dummy sentiment values (e.g., randomly\ntagging 20% as negative, 50% neutral, 30% positive) to prove the\nUI logic. That way, when the LLM integration went live,\neverything would ‚Äújust work.‚Äù\nOne rule I followed: validate every step against raw data. Each\nnew query or table, I ran the corresponding SQL directly in\nRedshift, exported counts, and cross-checked with what Retool\nshowed. Example: a video with 320 comments in raw data had to","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"30-30","partInfo":"Part 30 of 34"},"chunkIndex":29,"boundaries":{"startChar":0,"endChar":1678,"overlapWords":1,"splitAt":"paragraph_break"},"tokenCount":351},{"text":"Example: a video with 320 comments in raw data had to\n\nshow 320 in the dashboard (even if only 10 were visible by\ndefault). This habit prevented surprises later.\nI didn‚Äôt leave validation to my last days‚Äîit was part of my\ndaily rhythm.\nBy the end, this comments tab wasn‚Äôt just ‚Äúanother dashboard.‚Äù\nIt was our differentiator. Shortimize could tell you which video\nwas going viral. Our Retool dashboard could tell you why it was\ngoing viral‚Äîbecause users were laughing in comments, or because\nthey were tagging Hunch, or because they were comparing us to\nBumble. That qualitative layer made the dashboard not just\nanalytics, but a marketing intelligence tool.\nIn my final days, I wasn‚Äôt ‚Äúfinally‚Äù verifying SQL‚ÄîI‚Äôd been\nvalidating each component as I built it from day one. What I did\nnear the end was one more sweep: re-running my existing spot\nchecks on the most business-critical tables after some pipeline\nchanges, re-reading the comments in the queries, and noting\noptimization ideas I‚Äôd uncovered along the way. I also\nimplemented the button-label counts (‚ÄúVideos (100)‚Äù, ‚ÄúAccounts\n(8)‚Äù) to match Shortimize behavior, cleaned up hidden/unused\nelements, documented naming conventions (page-scoped prefixes,\nclear component and query names), and wrote a last to-do list:\nwhich columns still needed logic confirmation, and which data\nissues the data engineer should double-check. I refactored a\ncouple of earlier quick hacks using things I‚Äôd learned over the\nmonth so whoever picked it up had the best possible base.\nAs my tenure was ending, I handed this project to a new\nteammate. I walked him through the project: what we‚Äôre building,\nhow Retool is set up, the tricks I used, how to break complex\nproblems down, and which Retool concepts had tripped me up\n(actions, local storage, global vars, reuse patterns). I showed\nhow I build charts (x/y fields, multi-series, query binding). We\nrecorded the session, but one version had noise and the other\nhad good audio but no screen. I then combined sources to make a\nusable handover. The founders asked me to stay longer to finish\nsentiment prompts and the LLM wiring, but I was still recovering\npost-hospital and my AI cohort was starting in a week. My doctor\nhad advised rest, but founder-level projects don‚Äôt always allow\nthat. I was already working long hours and felt I had to stop. I\ndeclined, explained the medical reasons, and prepared the\ncleanest handover I could.","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"31-31","partInfo":"Part 31 of 34"},"chunkIndex":30,"boundaries":{"startChar":57227,"endChar":59650,"overlapWords":11,"splitAt":"paragraph_break"},"tokenCount":520},{"text":"I\ndeclined, explained the medical reasons, and prepared the\ncleanest handover I could.\n\nBefore leaving, I documented everything in detail:\n‚óè The SQL logic for parent‚Äìchild joins.\n‚óè Why Top-N selectors were added.\n‚óè How sentiment/mentions placeholders were set up.\n‚óè How to extend filters if new competitor names were added.\n‚óè Etc‚Ä¶‚Ä¶\nI also recorded a walkthrough for the new teammate, showing\nexactly how to debug comment mismatches or add new charts.\nThat‚Äôs where the Retool task wrapped for me.\nHandover\nWith just two days left in my notice period, I knew it was time\nto stop polishing edge cases and instead focus on what mattered\nmost: leaving behind a clean, structured handover. I reached out\nto the founders directly and told them openly that, while I\ncould keep refining the Retool dashboards endlessly, the more\nvaluable thing for the company would be for me to write out and\nrecord everything ‚Äî so the next person wouldn‚Äôt be left\nguessing. They agreed, and that‚Äôs how I switched gears from\nbuilding to documenting.\nI started with the Retool project, since it was the most\ncritical and the one where so many small decisions had been made\nalong the way. Even though I had already walked the new hire\nthrough the system in calls and shared recordings, I wanted a\nwritten hand-off document ‚Äî something that explained not just\nwhat was built, but also the reasoning, the edge cases, the\nshortcuts taken, and the to-do‚Äôs left behind. I detailed out the\ntwo big phases:\n‚óè Phase 1: Replicating Shortimize ‚Äì the overview dashboards,\nthe account-level metrics, the video filters, the global\nlogic.\n‚óè Phase 2: Extending with custom features ‚Äì especially the\ncomments dashboard, where we had gone beyond Shortimize and","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"32-32","partInfo":"Part 32 of 34"},"chunkIndex":31,"boundaries":{"startChar":59564,"endChar":61280,"overlapWords":13,"splitAt":"paragraph_break"},"tokenCount":377},{"text":"‚óè Phase 2: Extending with custom features ‚Äì especially the\ncomments dashboard, where we had gone beyond Shortimize and\n\nset the groundwork for sentiment analysis.\nFor every table and every chart, I explained the SQL query\nlogic, the dependencies, and the naming conventions I had used ‚Äî\nso the next person could follow a consistent system. I also\nflagged open items: certain columns where the founders still\nhadn‚Äôt aligned, places where the logic could be made more\nefficient, and polish tasks that weren‚Äôt strictly necessary for\nV1 but worth doing later.\nAfter finishing Retool‚Äôs handoff, I turned to content, which had\nbeen my other major vertical. Even though MBTI polls were being\nrolled out globally ‚Äî reducing the volume of manual content work\n‚Äî I wanted to make sure no one was caught off guard if something\nbackfired and we had to rely on older poll formats. Since my\nmanager had left abruptly a few days earlier, I no longer had\nanyone to buffer this responsibility; it was on me to leave the\nteam prepared.\nSo I wrote a content hand-off document as well. I included:\n‚óè The logic and usage of fixed polls, promoted polls, and\nMBTI polls.\n‚óè The system prompts for Short Vibe ‚Äî including the new\nupdates we had recently rolled out.\n‚óè Step-by-step instructions for using the admin dashboard\nportal: how to create campaigns, how to update or delete\nthem, what fields to fill carefully, what fields to avoid\ntouching.\n‚óè Notes on the recent changes in polls that required tweaks\nto Short Vibe content, so the next person wouldn‚Äôt miss\nthem.\nTo make this stick, I didn‚Äôt just hand over docs. I also\nscheduled a live handover call with senior management, walking\nthem through the portals, the dashboards, and the logic in real\ntime. I encouraged them to record the call ‚Äî and I also recorded\na local backup myself, just in case. There were technical\nhiccups (one recording had noise, another lost the screen), but","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"33-33","partInfo":"Part 33 of 34"},"chunkIndex":32,"boundaries":{"startChar":61162,"endChar":63076,"overlapWords":19,"splitAt":"paragraph_break"},"tokenCount":432},{"text":"There were technical\nhiccups (one recording had noise, another lost the screen), but\n\nI even went the extra mile to sync audio and video later, so at\nleast one complete reference would exist.\nBy the time I was done, I felt confident I had left no gaps.\nBetween the Retool documentation, the content hand-off, the\nrecorded walkthroughs, and the flagged to-dos, the company had\neverything it needed to keep moving without me.\nAnd then, on my final day, came something I didn‚Äôt expect. The\nfounder sent a farewell message on the company‚Äôs main slack\nchannel, describing my journey ‚Äî from content writer to AI APM,\nand leading AI-driven projects. They highlighted how, despite\nbeing hospitalized for over a week, I had come back and taken on\nthe Retool project, learned the tool from scratch, and delivered\na full-fledged marketing dashboard that replaced costly\nthird-party tools. They wrote that this was the first time in\nthe company‚Äôs history that someone would receive 150% of their\npromised bonus, and they rated my leadership ‚Äú4 out of 4.‚Äù\nWhat struck me most wasn‚Äôt the bonus ‚Äî though that was\nmeaningful ‚Äî but the recognition. The message closed by saying\nthey would personally write me a reference letter, commending my\nmaturity, technical growth, and the way I had used AI to\ntransform my role. Reading that message on my last day was\nsurreal. After three years of pouring myself into the work ‚Äî\nnights, weekends, experiments, and all ‚Äî it felt like the\nclosure I didn‚Äôt know I needed.","category":"journey","subcategory":null,"metadata":{"source":"journey_fy-2025-2026.pdf","fiscalYear":"FY25-26","paragraphRange":"34-34","partInfo":"Part 34 of 34"},"chunkIndex":33,"boundaries":{"startChar":62992,"endChar":64484,"overlapWords":13,"splitAt":"paragraph_break"},"tokenCount":332}],"timestamp":"2025-11-06T14:37:14.588Z"}